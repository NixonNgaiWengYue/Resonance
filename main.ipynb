{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3510c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comments_1 = pd.read_csv(r\"C:\\Users\\Nixon Ngai Weng Yue\\Downloads\\dataset\\comments1.csv\")\n",
    "comments_2 = pd.read_csv(r\"C:\\Users\\Nixon Ngai Weng Yue\\Downloads\\dataset\\comments2.csv\")\n",
    "comments_3 = pd.read_csv(r\"C:\\Users\\Nixon Ngai Weng Yue\\Downloads\\dataset\\comments3.csv\")\n",
    "comments_4 = pd.read_csv(r\"C:\\Users\\Nixon Ngai Weng Yue\\Downloads\\dataset\\comments4.csv\")\n",
    "comments_5 = pd.read_csv(r\"C:\\Users\\Nixon Ngai Weng Yue\\Downloads\\dataset\\comments5.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "06b37a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_video = pd.read_csv(r\"C:\\Users\\Nixon Ngai Weng Yue\\Downloads\\dataset\\videos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6a3b7566",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate([comments_1, comments_2, comments_3, comments_4,comments_5], start=1):\n",
    "    df[\"dataset_id\"] = i\n",
    "\n",
    "# Combine\n",
    "df_all_comments = pd.concat([comments_1, comments_2, comments_3, comments_4,comments_5], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b39b434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_soe(video_df, comment_df, w_video_like=1, w_fav=2, w_comment=3, w_comment_like=1):\n",
    "    # smoothing constant (avoid inflated scores for small videos)\n",
    "    k = int(video_df[\"viewCount\"].median())\n",
    "\n",
    "    video_df[\"publishedAt\"] = pd.to_datetime(video_df[\"publishedAt\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "    # 1. Aggregate comment-level data per video\n",
    "    comment_stats = comment_df.groupby(\"videoId\").agg(\n",
    "        comment_likes_sum=(\"likeCount\", \"sum\"),\n",
    "        comment_count=(\"commentId\", \"count\")\n",
    "    ).reset_index()\n",
    "\n",
    "    # 2. Merge into video-level df\n",
    "    merged = video_df.merge(comment_stats, on=\"videoId\", how=\"left\")\n",
    "\n",
    "    # Fill NA where videos had no comments\n",
    "    merged[\"comment_likes_sum\"] = merged[\"comment_likes_sum\"].fillna(0)\n",
    "    merged[\"comment_count\"] = merged[\"comment_count\"].fillna(0)\n",
    "\n",
    "    # 3. Engagement score\n",
    "    merged[\"engagement\"] = (\n",
    "        w_video_like * merged[\"likeCount\"] +\n",
    "        w_fav * merged[\"favouriteCount\"] +\n",
    "        w_comment * merged[\"commentCount\"] +\n",
    "        w_comment_like * merged[\"comment_likes_sum\"]\n",
    "    )\n",
    "\n",
    "    # 4. Denominator\n",
    "    merged[\"denominator\"] = merged[\"viewCount\"] + k\n",
    "\n",
    "    # 5. Final SoE (video level)\n",
    "    merged[\"SoE\"] = merged[\"engagement\"] / merged[\"denominator\"]\n",
    "\n",
    "    # 6. Broadcast SoE to comments (comment-level granularity)\n",
    "    comment_with_soe = comment_df.merge(\n",
    "        merged[[\"videoId\", \"SoE\"]],\n",
    "        on=\"videoId\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    return comment_with_soe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "faf1571b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               kind  commentId  channelId  videoId  authorId  \\\n",
      "0   youtube#comment    1781382      14492    74288   2032536   \n",
      "1   youtube#comment     289571      14727    79618   3043229   \n",
      "2   youtube#comment     569077       3314    51826    917006   \n",
      "3   youtube#comment    2957962       5008    58298   1853470   \n",
      "4   youtube#comment     673093      21411     1265   2584166   \n",
      "5   youtube#comment    1525723      18073    69091   1915402   \n",
      "6   youtube#comment    3079259       2988    91785    885833   \n",
      "7   youtube#comment     579871      45729    66477   2775194   \n",
      "8   youtube#comment    4596873      18670    88816   1040123   \n",
      "9   youtube#comment     910365      25235    31185   1092265   \n",
      "10  youtube#comment     779733      36205    72492   3158722   \n",
      "11  youtube#comment    1204214      33644    86518   1756840   \n",
      "12  youtube#comment     957058      20162    75828   1548100   \n",
      "13  youtube#comment    4672340      29550    53751   1064748   \n",
      "14  youtube#comment    2797490       4776    57274    590651   \n",
      "15  youtube#comment    3439642      42058    80982   3499981   \n",
      "16  youtube#comment    1055950      41246    55292    785546   \n",
      "17  youtube#comment    2230760      11003    28238    593180   \n",
      "18  youtube#comment     211328      17781    87279   2454363   \n",
      "19  youtube#comment    4185450      10928    48281   1352230   \n",
      "\n",
      "                                         textOriginal  parentCommentId  \\\n",
      "0   PLEASE LESBIAN FLAG I BEG YOU \\n\\nYou would ro...              NaN   \n",
      "1    Apply mashed potato juice and mixed it with curd        3198066.0   \n",
      "2                          69 missed calls from marsüëΩ              NaN   \n",
      "3                                                Baaa              NaN   \n",
      "4     you look like raven from phenomena raven no cap              NaN   \n",
      "5                                            American              NaN   \n",
      "6          Sahi disha me ja ja raha india ka Future..              NaN   \n",
      "7                                          ‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§              NaN   \n",
      "8                     Love your videos. Thank you ‚ù§‚ù§‚ù§              NaN   \n",
      "9   India is  the best and  very beautiful üòçüòçüòçüòçüòçüòçüòç...              NaN   \n",
      "10                                                  ‚ù§              NaN   \n",
      "11  This is so true. Skinny jeans always nipped meüò≠üò≠üò≠              NaN   \n",
      "12                                 Dunia semakin aneh              NaN   \n",
      "13                 I love korean and japanese makeup!              NaN   \n",
      "14     Plz upload a vedio about your teeth transition              NaN   \n",
      "15  Why don‚Äôt you use wigs? Or is it just better t...              NaN   \n",
      "16                                               Love              NaN   \n",
      "17  What hair oil did u use because the front of m...              NaN   \n",
      "18  Oh, I'm so glad that you found this channel th...        3686870.0   \n",
      "19                                 Wavy plus straight              NaN   \n",
      "\n",
      "    likeCount                publishedAt                  updatedAt  \\\n",
      "0           0  2023-08-15 21:48:52+00:00  2023-08-15 21:48:52+00:00   \n",
      "1           0  2023-10-02 13:08:22+00:00  2023-10-02 13:08:22+00:00   \n",
      "2           0  2024-05-31 12:03:12+00:00  2024-05-31 12:03:12+00:00   \n",
      "3           0  2024-02-13 15:48:37+00:00  2024-02-13 15:48:37+00:00   \n",
      "4           0  2020-02-15 22:28:44+00:00  2020-02-15 22:28:44+00:00   \n",
      "5           0  2023-01-17 12:50:08+00:00  2023-01-17 12:50:08+00:00   \n",
      "6           0  2021-09-03 06:51:48+00:00  2021-09-03 06:51:48+00:00   \n",
      "7           0  2025-01-17 22:08:38+00:00  2025-01-17 22:08:38+00:00   \n",
      "8           1  2023-06-24 07:24:06+00:00  2023-06-24 07:24:06+00:00   \n",
      "9           0  2025-02-27 14:49:22+00:00  2025-02-27 14:49:22+00:00   \n",
      "10          1  2023-04-11 07:02:06+00:00  2023-04-11 07:02:06+00:00   \n",
      "11          1  2023-06-16 15:14:54+00:00  2023-06-16 15:14:54+00:00   \n",
      "12          0  2023-04-28 04:13:47+00:00  2023-04-28 04:13:47+00:00   \n",
      "13          0  2025-01-25 05:27:14+00:00  2025-01-25 05:27:14+00:00   \n",
      "14          0  2025-02-07 03:57:52+00:00  2025-02-07 03:57:52+00:00   \n",
      "15          0  2025-01-24 18:31:34+00:00  2025-01-24 18:31:34+00:00   \n",
      "16          1  2022-05-08 07:01:29+00:00  2022-05-08 07:01:29+00:00   \n",
      "17          0  2024-08-08 01:47:19+00:00  2024-08-08 01:47:19+00:00   \n",
      "18          1  2023-05-09 04:40:28+00:00  2023-05-09 04:40:28+00:00   \n",
      "19          0  2025-05-29 19:04:23+00:00  2025-05-29 19:04:23+00:00   \n",
      "\n",
      "    dataset_id       SoE  \n",
      "0            1  0.034254  \n",
      "1            1  0.048488  \n",
      "2            1  0.027266  \n",
      "3            1  0.086262  \n",
      "4            1  0.089835  \n",
      "5            1  0.034362  \n",
      "6            1  0.017975  \n",
      "7            1  0.035624  \n",
      "8            1  0.016916  \n",
      "9            1  0.034845  \n",
      "10           1  0.029876  \n",
      "11           1  0.071154  \n",
      "12           1       NaN  \n",
      "13           1  0.089267  \n",
      "14           1  0.040324  \n",
      "15           1  0.031497  \n",
      "16           1  0.048882  \n",
      "17           1  0.054947  \n",
      "18           1  0.053568  \n",
      "19           1  0.037430  \n"
     ]
    }
   ],
   "source": [
    "result_df = compute_soe(df_video, df_all_comments)\n",
    "\n",
    "print(result_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a93e8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment_depth(comment_id, parent_map):\n",
    "    \"\"\"Compute depth of a single comment given a parent mapping.\"\"\"\n",
    "    depth = 0\n",
    "    parent = parent_map.get(comment_id, None)\n",
    "    while parent is not None:\n",
    "        depth += 1\n",
    "        parent = parent_map.get(parent, None)\n",
    "    return depth\n",
    "\n",
    "\n",
    "def compute_depths(df):\n",
    "    \"\"\"Add a depth column to the DataFrame.\"\"\"\n",
    "    df = df.copy()\n",
    "    if \"commentId\" not in df.columns or \"parentCommentId\" not in df.columns:\n",
    "        raise KeyError(\"DataFrame must contain 'commentId' and 'parentCommentId' columns\")\n",
    "    parent_map = df.set_index(\"commentId\")[\"parentCommentId\"].to_dict()\n",
    "    df[\"depth\"] = df[\"commentId\"].apply(lambda cid: get_comment_depth(cid, parent_map))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "61c24908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               kind  commentId  channelId  videoId  authorId  \\\n",
      "0   youtube#comment    1781382      14492    74288   2032536   \n",
      "1   youtube#comment     289571      14727    79618   3043229   \n",
      "2   youtube#comment     569077       3314    51826    917006   \n",
      "3   youtube#comment    2957962       5008    58298   1853470   \n",
      "4   youtube#comment     673093      21411     1265   2584166   \n",
      "5   youtube#comment    1525723      18073    69091   1915402   \n",
      "6   youtube#comment    3079259       2988    91785    885833   \n",
      "7   youtube#comment     579871      45729    66477   2775194   \n",
      "8   youtube#comment    4596873      18670    88816   1040123   \n",
      "9   youtube#comment     910365      25235    31185   1092265   \n",
      "10  youtube#comment     779733      36205    72492   3158722   \n",
      "11  youtube#comment    1204214      33644    86518   1756840   \n",
      "12  youtube#comment     957058      20162    75828   1548100   \n",
      "13  youtube#comment    4672340      29550    53751   1064748   \n",
      "14  youtube#comment    2797490       4776    57274    590651   \n",
      "15  youtube#comment    3439642      42058    80982   3499981   \n",
      "16  youtube#comment    1055950      41246    55292    785546   \n",
      "17  youtube#comment    2230760      11003    28238    593180   \n",
      "18  youtube#comment     211328      17781    87279   2454363   \n",
      "19  youtube#comment    4185450      10928    48281   1352230   \n",
      "\n",
      "                                         textOriginal  parentCommentId  \\\n",
      "0   PLEASE LESBIAN FLAG I BEG YOU \\n\\nYou would ro...              NaN   \n",
      "1    Apply mashed potato juice and mixed it with curd        3198066.0   \n",
      "2                          69 missed calls from marsüëΩ              NaN   \n",
      "3                                                Baaa              NaN   \n",
      "4     you look like raven from phenomena raven no cap              NaN   \n",
      "5                                            American              NaN   \n",
      "6          Sahi disha me ja ja raha india ka Future..              NaN   \n",
      "7                                          ‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§              NaN   \n",
      "8                     Love your videos. Thank you ‚ù§‚ù§‚ù§              NaN   \n",
      "9   India is  the best and  very beautiful üòçüòçüòçüòçüòçüòçüòç...              NaN   \n",
      "10                                                  ‚ù§              NaN   \n",
      "11  This is so true. Skinny jeans always nipped meüò≠üò≠üò≠              NaN   \n",
      "12                                 Dunia semakin aneh              NaN   \n",
      "13                 I love korean and japanese makeup!              NaN   \n",
      "14     Plz upload a vedio about your teeth transition              NaN   \n",
      "15  Why don‚Äôt you use wigs? Or is it just better t...              NaN   \n",
      "16                                               Love              NaN   \n",
      "17  What hair oil did u use because the front of m...              NaN   \n",
      "18  Oh, I'm so glad that you found this channel th...        3686870.0   \n",
      "19                                 Wavy plus straight              NaN   \n",
      "\n",
      "    likeCount                publishedAt                  updatedAt  \\\n",
      "0           0  2023-08-15 21:48:52+00:00  2023-08-15 21:48:52+00:00   \n",
      "1           0  2023-10-02 13:08:22+00:00  2023-10-02 13:08:22+00:00   \n",
      "2           0  2024-05-31 12:03:12+00:00  2024-05-31 12:03:12+00:00   \n",
      "3           0  2024-02-13 15:48:37+00:00  2024-02-13 15:48:37+00:00   \n",
      "4           0  2020-02-15 22:28:44+00:00  2020-02-15 22:28:44+00:00   \n",
      "5           0  2023-01-17 12:50:08+00:00  2023-01-17 12:50:08+00:00   \n",
      "6           0  2021-09-03 06:51:48+00:00  2021-09-03 06:51:48+00:00   \n",
      "7           0  2025-01-17 22:08:38+00:00  2025-01-17 22:08:38+00:00   \n",
      "8           1  2023-06-24 07:24:06+00:00  2023-06-24 07:24:06+00:00   \n",
      "9           0  2025-02-27 14:49:22+00:00  2025-02-27 14:49:22+00:00   \n",
      "10          1  2023-04-11 07:02:06+00:00  2023-04-11 07:02:06+00:00   \n",
      "11          1  2023-06-16 15:14:54+00:00  2023-06-16 15:14:54+00:00   \n",
      "12          0  2023-04-28 04:13:47+00:00  2023-04-28 04:13:47+00:00   \n",
      "13          0  2025-01-25 05:27:14+00:00  2025-01-25 05:27:14+00:00   \n",
      "14          0  2025-02-07 03:57:52+00:00  2025-02-07 03:57:52+00:00   \n",
      "15          0  2025-01-24 18:31:34+00:00  2025-01-24 18:31:34+00:00   \n",
      "16          1  2022-05-08 07:01:29+00:00  2022-05-08 07:01:29+00:00   \n",
      "17          0  2024-08-08 01:47:19+00:00  2024-08-08 01:47:19+00:00   \n",
      "18          1  2023-05-09 04:40:28+00:00  2023-05-09 04:40:28+00:00   \n",
      "19          0  2025-05-29 19:04:23+00:00  2025-05-29 19:04:23+00:00   \n",
      "\n",
      "    dataset_id  depth  \n",
      "0            1      1  \n",
      "1            1      2  \n",
      "2            1      1  \n",
      "3            1      1  \n",
      "4            1      1  \n",
      "5            1      1  \n",
      "6            1      1  \n",
      "7            1      1  \n",
      "8            1      1  \n",
      "9            1      1  \n",
      "10           1      1  \n",
      "11           1      1  \n",
      "12           1      1  \n",
      "13           1      1  \n",
      "14           1      1  \n",
      "15           1      1  \n",
      "16           1      1  \n",
      "17           1      1  \n",
      "18           1      2  \n",
      "19           1      1  \n"
     ]
    }
   ],
   "source": [
    "result_compute_depths = compute_depths(df_all_comments)\n",
    "print(result_compute_depths.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "fca30fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_reply_factor(df, depth_weight=0.5):\n",
    "    df = df.copy()\n",
    "    if \"depth\" not in df.columns:\n",
    "        df = compute_depths(df)\n",
    "\n",
    "    # Count number of replies for each comment\n",
    "    reply_counts = df[\"parentCommentId\"].value_counts().to_dict()\n",
    "    df[\"numReplies\"] = df[\"commentId\"].map(reply_counts).fillna(0).astype(int)\n",
    "\n",
    "    # ReplyFactor = depth scaling + log of number of replies\n",
    "    df[\"ReplyFactor\"] = (1 + depth_weight * df[\"depth\"]) * (1 + np.log1p(df[\"numReplies\"]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "66e0756f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4725012 entries, 0 to 4725011\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Dtype  \n",
      "---  ------           -----  \n",
      " 0   kind             object \n",
      " 1   commentId        int64  \n",
      " 2   channelId        int64  \n",
      " 3   videoId          int64  \n",
      " 4   authorId         int64  \n",
      " 5   textOriginal     object \n",
      " 6   parentCommentId  float64\n",
      " 7   likeCount        int64  \n",
      " 8   publishedAt      object \n",
      " 9   updatedAt        object \n",
      " 10  dataset_id       int64  \n",
      "dtypes: float64(1), int64(6), object(4)\n",
      "memory usage: 396.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_all_comments.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e73e3f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              kind  commentId  channelId  videoId  authorId  \\\n",
      "0  youtube#comment    1781382      14492    74288   2032536   \n",
      "1  youtube#comment     289571      14727    79618   3043229   \n",
      "2  youtube#comment     569077       3314    51826    917006   \n",
      "3  youtube#comment    2957962       5008    58298   1853470   \n",
      "4  youtube#comment     673093      21411     1265   2584166   \n",
      "\n",
      "                                        textOriginal  parentCommentId  \\\n",
      "0  PLEASE LESBIAN FLAG I BEG YOU \\n\\nYou would ro...              NaN   \n",
      "1   Apply mashed potato juice and mixed it with curd        3198066.0   \n",
      "2                         69 missed calls from marsüëΩ              NaN   \n",
      "3                                               Baaa              NaN   \n",
      "4    you look like raven from phenomena raven no cap              NaN   \n",
      "\n",
      "   likeCount                publishedAt                  updatedAt  \\\n",
      "0          0  2023-08-15 21:48:52+00:00  2023-08-15 21:48:52+00:00   \n",
      "1          0  2023-10-02 13:08:22+00:00  2023-10-02 13:08:22+00:00   \n",
      "2          0  2024-05-31 12:03:12+00:00  2024-05-31 12:03:12+00:00   \n",
      "3          0  2024-02-13 15:48:37+00:00  2024-02-13 15:48:37+00:00   \n",
      "4          0  2020-02-15 22:28:44+00:00  2020-02-15 22:28:44+00:00   \n",
      "\n",
      "   dataset_id  depth  numReplies  ReplyFactor  \n",
      "0           1      1           1     2.539721  \n",
      "1           1      2           0     2.000000  \n",
      "2           1      1           0     1.500000  \n",
      "3           1      1           0     1.500000  \n",
      "4           1      1           0     1.500000  \n"
     ]
    }
   ],
   "source": [
    "result_reply_factor = compute_reply_factor(df_all_comments,depth_weight=0.5)\n",
    "\n",
    "print(result_reply_factor.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "5976187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_relevance_by_video(video_df, comment_df, text_col=\"textOriginal\",\n",
    "                                 video_meta_cols=(\"title\",\"description\",\"tags\")):\n",
    "    \"\"\"\n",
    "    Compute relevance (cosine similarity) between each comment and its video's metadata.\n",
    "    Strategy: process per videoId to avoid building a huge global matrix.\n",
    "    Returns: comment_df copy with added \"relevance\" column (float in [0,1]).\n",
    "    \"\"\"\n",
    "    comment_df = comment_df.copy()\n",
    "\n",
    "    # Ensure expected columns exist\n",
    "    for c in (\"videoId\", text_col, \"commentId\"):\n",
    "        if c not in comment_df.columns:\n",
    "            raise KeyError(f\"comment_df must contain column '{c}'\")\n",
    "    for c in (\"videoId\",) + video_meta_cols:\n",
    "        if c not in video_df.columns:\n",
    "            raise KeyError(f\"video_df must contain column '{c}'\")\n",
    "\n",
    "    # Build video metadata text map\n",
    "    video_df = video_df.copy()\n",
    "    video_df[\"video_meta\"] = (\n",
    "        video_df[video_meta_cols[0]].fillna(\"\").astype(str) + \" \" +\n",
    "        video_df[video_meta_cols[1]].fillna(\"\").astype(str) + \" \" +\n",
    "        video_df[video_meta_cols[2]].fillna(\"\").astype(str)\n",
    "    )\n",
    "    video_meta_map = dict(zip(video_df[\"videoId\"], video_df[\"video_meta\"]))\n",
    "\n",
    "    # Prepare output container\n",
    "    relevances = np.zeros(len(comment_df), dtype=float)\n",
    "\n",
    "    # iterate grouped by videoId\n",
    "    for vid, group in comment_df.groupby(\"videoId\", sort=False):\n",
    "        idx = group.index  # indices in original comment_df\n",
    "        video_text = str(video_meta_map.get(vid, \"\")).strip()\n",
    "        # collect comment texts\n",
    "        comments = group[text_col].fillna(\"\").astype(str).str.strip().tolist()\n",
    "\n",
    "        # If video metadata or all comments empty -> fallback 0\n",
    "        if (not video_text) or all(not c for c in comments):\n",
    "            continue  # already initialized to 0.0\n",
    "\n",
    "        try:\n",
    "            # Build TF-IDF on (video_meta + comments)\n",
    "            texts = comments + [video_text]   # last element is video\n",
    "            vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "            vectors = vectorizer.fit_transform(texts)  # sparse matrix\n",
    "            comment_vecs = vectors[:-1]   # shape (n_comments, n_features)\n",
    "            video_vec = vectors[-1]       # shape (1, n_features)\n",
    "\n",
    "            # cosine similarity between each comment vector and video vector\n",
    "            sims = cosine_similarity(comment_vecs, video_vec).reshape(-1)\n",
    "            sims = np.clip(sims, 0.0, 1.0)\n",
    "            relevances[idx] = sims\n",
    "\n",
    "        except ValueError:\n",
    "            # e.g., empty vocabulary (all stopwords)\n",
    "            relevances[idx] = 0.0\n",
    "\n",
    "    comment_df[\"relevance\"] = relevances\n",
    "    return comment_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d2c744f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              kind  commentId  channelId  videoId  authorId  \\\n",
      "0  youtube#comment    1781382      14492    74288   2032536   \n",
      "1  youtube#comment     289571      14727    79618   3043229   \n",
      "2  youtube#comment     569077       3314    51826    917006   \n",
      "3  youtube#comment    2957962       5008    58298   1853470   \n",
      "4  youtube#comment     673093      21411     1265   2584166   \n",
      "\n",
      "                                        textOriginal  parentCommentId  \\\n",
      "0  PLEASE LESBIAN FLAG I BEG YOU \\n\\nYou would ro...              NaN   \n",
      "1   Apply mashed potato juice and mixed it with curd        3198066.0   \n",
      "2                         69 missed calls from marsüëΩ              NaN   \n",
      "3                                               Baaa              NaN   \n",
      "4    you look like raven from phenomena raven no cap              NaN   \n",
      "\n",
      "   likeCount                publishedAt                  updatedAt  \\\n",
      "0          0  2023-08-15 21:48:52+00:00  2023-08-15 21:48:52+00:00   \n",
      "1          0  2023-10-02 13:08:22+00:00  2023-10-02 13:08:22+00:00   \n",
      "2          0  2024-05-31 12:03:12+00:00  2024-05-31 12:03:12+00:00   \n",
      "3          0  2024-02-13 15:48:37+00:00  2024-02-13 15:48:37+00:00   \n",
      "4          0  2020-02-15 22:28:44+00:00  2020-02-15 22:28:44+00:00   \n",
      "\n",
      "   dataset_id  relevance  \n",
      "0           1   0.076233  \n",
      "1           1   0.008955  \n",
      "2           1   0.000000  \n",
      "3           1   0.000000  \n",
      "4           1   0.001714  \n"
     ]
    }
   ],
   "source": [
    "result_relevance = calculate_relevance_by_video(df_video,df_all_comments)\n",
    "print(result_relevance.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c9595cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          commentId     channelId       videoId      authorId  \\\n",
      "count  4.725012e+06  4.725012e+06  4.725012e+06  4.725012e+06   \n",
      "mean   2.362507e+06  2.677102e+04  4.699673e+04  1.820729e+06   \n",
      "std    1.363995e+06  1.503666e+04  2.593627e+04  1.053868e+06   \n",
      "min    0.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    1.181254e+06  1.449200e+04  2.597800e+04  9.060968e+05   \n",
      "50%    2.362506e+06  2.542500e+04  4.709600e+04  1.813632e+06   \n",
      "75%    3.543761e+06  4.061800e+04  6.944500e+04  2.731818e+06   \n",
      "max    4.725015e+06  5.367700e+04  9.285400e+04  3.659440e+06   \n",
      "\n",
      "       parentCommentId     likeCount    dataset_id     relevance  \n",
      "count     5.161350e+05  4.725012e+06  4.725012e+06  4.725012e+06  \n",
      "mean      2.623663e+06  1.012744e+01  2.883605e+00  2.485385e-02  \n",
      "std       1.215428e+06  5.444689e+02  1.367501e+00  5.162430e-02  \n",
      "min       5.161510e+05  0.000000e+00  1.000000e+00  0.000000e+00  \n",
      "25%       1.569545e+06  0.000000e+00  2.000000e+00  0.000000e+00  \n",
      "50%       2.625877e+06  0.000000e+00  3.000000e+00  0.000000e+00  \n",
      "75%       3.677645e+06  0.000000e+00  4.000000e+00  3.117165e-02  \n",
      "max       4.725006e+06  4.561420e+05  5.000000e+00  1.000000e+00  \n"
     ]
    }
   ],
   "source": [
    "print(result_relevance.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4c8315fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          commentId     channelId       videoId      authorId  \\\n",
      "count  4.725012e+06  4.725012e+06  4.725012e+06  4.725012e+06   \n",
      "mean   2.362507e+06  2.677102e+04  4.699673e+04  1.820729e+06   \n",
      "std    1.363995e+06  1.503666e+04  2.593627e+04  1.053868e+06   \n",
      "min    0.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    1.181254e+06  1.449200e+04  2.597800e+04  9.060968e+05   \n",
      "50%    2.362506e+06  2.542500e+04  4.709600e+04  1.813632e+06   \n",
      "75%    3.543761e+06  4.061800e+04  6.944500e+04  2.731818e+06   \n",
      "max    4.725015e+06  5.367700e+04  9.285400e+04  3.659440e+06   \n",
      "\n",
      "       parentCommentId     likeCount    dataset_id         depth  \n",
      "count     5.161350e+05  4.725012e+06  4.725012e+06  4.725012e+06  \n",
      "mean      2.623663e+06  1.012744e+01  2.883605e+00  1.109235e+00  \n",
      "std       1.215428e+06  5.444689e+02  1.367501e+00  3.119334e-01  \n",
      "min       5.161510e+05  0.000000e+00  1.000000e+00  1.000000e+00  \n",
      "25%       1.569545e+06  0.000000e+00  2.000000e+00  1.000000e+00  \n",
      "50%       2.625877e+06  0.000000e+00  3.000000e+00  1.000000e+00  \n",
      "75%       3.677645e+06  0.000000e+00  4.000000e+00  1.000000e+00  \n",
      "max       4.725006e+06  4.561420e+05  5.000000e+00  2.000000e+00  \n"
     ]
    }
   ],
   "source": [
    "print(result_compute_depths.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "aa56ecb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parentCommentId\n",
      "NaN          4208877\n",
      "3479586.0          6\n",
      "3945948.0          6\n",
      "3589983.0          6\n",
      "1020618.0          6\n",
      "              ...   \n",
      "2080065.0          1\n",
      "1211588.0          1\n",
      "2710703.0          1\n",
      "2339374.0          1\n",
      "1672112.0          1\n",
      "Name: count, Length: 317070, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "parent_id_count = result_compute_depths[\"parentCommentId\"].value_counts(dropna = False)\n",
    "print(parent_id_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "5982ed41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4725012 entries, 0 to 4725011\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Dtype  \n",
      "---  ------           -----  \n",
      " 0   kind             object \n",
      " 1   commentId        int64  \n",
      " 2   channelId        int64  \n",
      " 3   videoId          int64  \n",
      " 4   authorId         int64  \n",
      " 5   textOriginal     object \n",
      " 6   parentCommentId  float64\n",
      " 7   likeCount        int64  \n",
      " 8   publishedAt      object \n",
      " 9   updatedAt        object \n",
      " 10  dataset_id       int64  \n",
      " 11  SoE              float64\n",
      "dtypes: float64(2), int64(6), object(4)\n",
      "memory usage: 432.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(result_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6b944e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time_norm(video_df, comment_df, tz=\"UTC\"):\n",
    "    today = pd.Timestamp.now(tz=tz)\n",
    "\n",
    "    # Ensure datetime types\n",
    "    video_df = video_df.copy()\n",
    "    comment_df = comment_df.copy()\n",
    "    video_df[\"publishedAt\"] = pd.to_datetime(video_df[\"publishedAt\"], errors=\"coerce\", utc=True)\n",
    "    comment_df[\"publishedAt\"] = pd.to_datetime(comment_df[\"publishedAt\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "    # Compute days since each comment\n",
    "    comment_df[\"days_since_comment\"] = (today - comment_df[\"publishedAt\"]).dt.days\n",
    "\n",
    "    # Normalize comment recency\n",
    "    max_days_comment = comment_df[\"days_since_comment\"].max()\n",
    "\n",
    "    if max_days_comment == 0 or pd.isna(max_days_comment):\n",
    "        comment_df[\"timeNorm\"] = 1.0\n",
    "    else:\n",
    "        comment_df[\"timeNorm\"] = 1 - (comment_df[\"days_since_comment\"] / max_days_comment)\n",
    "        comment_df[\"timeNorm\"] = comment_df[\"timeNorm\"].fillna(1.0)\n",
    "\n",
    "    return comment_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "fdcf4b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               kind  commentId  channelId  videoId  authorId  \\\n",
      "0   youtube#comment    1781382      14492    74288   2032536   \n",
      "1   youtube#comment     289571      14727    79618   3043229   \n",
      "2   youtube#comment     569077       3314    51826    917006   \n",
      "3   youtube#comment    2957962       5008    58298   1853470   \n",
      "4   youtube#comment     673093      21411     1265   2584166   \n",
      "5   youtube#comment    1525723      18073    69091   1915402   \n",
      "6   youtube#comment    3079259       2988    91785    885833   \n",
      "7   youtube#comment     579871      45729    66477   2775194   \n",
      "8   youtube#comment    4596873      18670    88816   1040123   \n",
      "9   youtube#comment     910365      25235    31185   1092265   \n",
      "10  youtube#comment     779733      36205    72492   3158722   \n",
      "11  youtube#comment    1204214      33644    86518   1756840   \n",
      "12  youtube#comment     957058      20162    75828   1548100   \n",
      "13  youtube#comment    4672340      29550    53751   1064748   \n",
      "14  youtube#comment    2797490       4776    57274    590651   \n",
      "15  youtube#comment    3439642      42058    80982   3499981   \n",
      "16  youtube#comment    1055950      41246    55292    785546   \n",
      "17  youtube#comment    2230760      11003    28238    593180   \n",
      "18  youtube#comment     211328      17781    87279   2454363   \n",
      "19  youtube#comment    4185450      10928    48281   1352230   \n",
      "\n",
      "                                         textOriginal  parentCommentId  \\\n",
      "0   PLEASE LESBIAN FLAG I BEG YOU \\n\\nYou would ro...              NaN   \n",
      "1    Apply mashed potato juice and mixed it with curd        3198066.0   \n",
      "2                          69 missed calls from marsüëΩ              NaN   \n",
      "3                                                Baaa              NaN   \n",
      "4     you look like raven from phenomena raven no cap              NaN   \n",
      "5                                            American              NaN   \n",
      "6          Sahi disha me ja ja raha india ka Future..              NaN   \n",
      "7                                          ‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§              NaN   \n",
      "8                     Love your videos. Thank you ‚ù§‚ù§‚ù§              NaN   \n",
      "9   India is  the best and  very beautiful üòçüòçüòçüòçüòçüòçüòç...              NaN   \n",
      "10                                                  ‚ù§              NaN   \n",
      "11  This is so true. Skinny jeans always nipped meüò≠üò≠üò≠              NaN   \n",
      "12                                 Dunia semakin aneh              NaN   \n",
      "13                 I love korean and japanese makeup!              NaN   \n",
      "14     Plz upload a vedio about your teeth transition              NaN   \n",
      "15  Why don‚Äôt you use wigs? Or is it just better t...              NaN   \n",
      "16                                               Love              NaN   \n",
      "17  What hair oil did u use because the front of m...              NaN   \n",
      "18  Oh, I'm so glad that you found this channel th...        3686870.0   \n",
      "19                                 Wavy plus straight              NaN   \n",
      "\n",
      "    likeCount               publishedAt                  updatedAt  \\\n",
      "0           0 2023-08-15 21:48:52+00:00  2023-08-15 21:48:52+00:00   \n",
      "1           0 2023-10-02 13:08:22+00:00  2023-10-02 13:08:22+00:00   \n",
      "2           0 2024-05-31 12:03:12+00:00  2024-05-31 12:03:12+00:00   \n",
      "3           0 2024-02-13 15:48:37+00:00  2024-02-13 15:48:37+00:00   \n",
      "4           0 2020-02-15 22:28:44+00:00  2020-02-15 22:28:44+00:00   \n",
      "5           0 2023-01-17 12:50:08+00:00  2023-01-17 12:50:08+00:00   \n",
      "6           0 2021-09-03 06:51:48+00:00  2021-09-03 06:51:48+00:00   \n",
      "7           0 2025-01-17 22:08:38+00:00  2025-01-17 22:08:38+00:00   \n",
      "8           1 2023-06-24 07:24:06+00:00  2023-06-24 07:24:06+00:00   \n",
      "9           0 2025-02-27 14:49:22+00:00  2025-02-27 14:49:22+00:00   \n",
      "10          1 2023-04-11 07:02:06+00:00  2023-04-11 07:02:06+00:00   \n",
      "11          1 2023-06-16 15:14:54+00:00  2023-06-16 15:14:54+00:00   \n",
      "12          0 2023-04-28 04:13:47+00:00  2023-04-28 04:13:47+00:00   \n",
      "13          0 2025-01-25 05:27:14+00:00  2025-01-25 05:27:14+00:00   \n",
      "14          0 2025-02-07 03:57:52+00:00  2025-02-07 03:57:52+00:00   \n",
      "15          0 2025-01-24 18:31:34+00:00  2025-01-24 18:31:34+00:00   \n",
      "16          1 2022-05-08 07:01:29+00:00  2022-05-08 07:01:29+00:00   \n",
      "17          0 2024-08-08 01:47:19+00:00  2024-08-08 01:47:19+00:00   \n",
      "18          1 2023-05-09 04:40:28+00:00  2023-05-09 04:40:28+00:00   \n",
      "19          0 2025-05-29 19:04:23+00:00  2025-05-29 19:04:23+00:00   \n",
      "\n",
      "    dataset_id  days_since_comment  timeNorm  \n",
      "0            1                 756  0.636189  \n",
      "1            1                 708  0.659288  \n",
      "2            1                 466  0.775746  \n",
      "3            1                 574  0.723773  \n",
      "4            1                2033  0.021655  \n",
      "5            1                 966  0.535130  \n",
      "6            1                1468  0.293551  \n",
      "7            1                 235  0.886910  \n",
      "8            1                 809  0.610683  \n",
      "9            1                 194  0.906641  \n",
      "10           1                 883  0.575072  \n",
      "11           1                 816  0.607315  \n",
      "12           1                 866  0.583253  \n",
      "13           1                 228  0.890279  \n",
      "14           1                 215  0.896535  \n",
      "15           1                 228  0.890279  \n",
      "16           1                1221  0.412416  \n",
      "17           1                 398  0.808470  \n",
      "18           1                 855  0.588547  \n",
      "19           1                 103  0.950433  \n"
     ]
    }
   ],
   "source": [
    "result_time_norm = compute_time_norm(df_video,df_all_comments,tz=\"UTC\")\n",
    "print(result_time_norm.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ca4ba7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "def train_weights(df, target_col=\"likeCount\", log_transform=True, cv_folds=5):\n",
    "    required_cols = [\"SoE\", \"ReplyFactor\", \"relevance\"]\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    for col in required_cols + [target_col]:\n",
    "        if col not in df.columns:\n",
    "            raise KeyError(f\"Column '{col}' is missing from dataframe\")\n",
    "\n",
    "    # Fill missing values\n",
    "    X = df[required_cols].fillna(0)\n",
    "    y = df[target_col].fillna(0)\n",
    "\n",
    "    # Optional log-transform\n",
    "    if log_transform:\n",
    "        y = np.log1p(y)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Train model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_scaled, y)\n",
    "\n",
    "    # Cross-validation (R¬≤ score)\n",
    "    cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring=\"r2\")\n",
    "\n",
    "    # Extract weights\n",
    "    results = pd.DataFrame([\n",
    "        {\"Item\": \"Model\", \"Value\": model},\n",
    "        {\"Item\": \"Scaler\", \"Value\": scaler},\n",
    "        {\"Item\": \"Intercept\", \"Value\": model.intercept_},\n",
    "        {\"Item\": \"CV_Scores\", \"Value\": cv_scores},\n",
    "        {\"Item\": \"CV_Mean\", \"Value\": np.mean(cv_scores)},\n",
    "    ])\n",
    "\n",
    "    # Append weights row by row\n",
    "    for feature, weight in zip(required_cols, model.coef_):\n",
    "        results = pd.concat([results, pd.DataFrame([{\"Item\": feature, \"Value\": weight}])],\n",
    "                            ignore_index=True)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fbfe2749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_results(soe_results_df, reply_factor_results_df, relevance_relevance_df,time_norm_df):\n",
    "    merged = soe_results_df.copy()\n",
    "    merged = merged.merge(\n",
    "        reply_factor_results_df[[\"commentId\", \"ReplyFactor\"]],\n",
    "        on=\"commentId\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    merged = merged.merge(\n",
    "        relevance_relevance_df[[\"commentId\", \"relevance\"]],\n",
    "        on=\"commentId\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    merged = merged.merge(\n",
    "        time_norm_df[[\"commentId\", \"timeNorm\"]],\n",
    "        on=\"commentId\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f6829cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4725012 entries, 0 to 4725011\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Dtype  \n",
      "---  ------           -----  \n",
      " 0   kind             object \n",
      " 1   commentId        int64  \n",
      " 2   channelId        int64  \n",
      " 3   videoId          int64  \n",
      " 4   authorId         int64  \n",
      " 5   textOriginal     object \n",
      " 6   parentCommentId  float64\n",
      " 7   likeCount        int64  \n",
      " 8   publishedAt      object \n",
      " 9   updatedAt        object \n",
      " 10  dataset_id       int64  \n",
      " 11  SoE              float64\n",
      " 12  ReplyFactor      float64\n",
      " 13  relevance        float64\n",
      " 14  timeNorm         float64\n",
      "dtypes: float64(5), int64(6), object(4)\n",
      "memory usage: 540.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "merged_df = merge_all_results(result_df, result_reply_factor, result_relevance,result_time_norm)\n",
    "print(merged_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "e343dc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008940814968675291\n",
      "Index(['Item', 'Value'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "results_weight = train_weights(merged_df, target_col=\"likeCount\")\n",
    "soe_value = results_weight.loc[results_weight[\"Item\"] == \"SoE\", \"Value\"].values[0]\n",
    "print(soe_value)\n",
    "print(results_weight.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ed9aabc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_score(merged_df, weights):\n",
    "    \"\"\"\n",
    "    Compute final score using learned weights from regression.\n",
    "\n",
    "    Formula:\n",
    "        FinalScore = intercept + (a*SoE + b*ReplyFactor + c*Relevance) * (TimeNorm if enabled)\n",
    "\n",
    "    Parameters:\n",
    "        merged_df : pd.DataFrame\n",
    "            Must contain columns: SoE, ReplyFactor, relevance\n",
    "        weights : dict\n",
    "            Learned weights, e.g. {\"SoE\": 0.0089, \"ReplyFactor\": 0.3491, \"relevance\": 0.0269}\n",
    "        intercept : float\n",
    "            Learned intercept (default=0.0 if ignored)\n",
    "        use_time_norm : bool\n",
    "            Whether to multiply by TimeNorm if available in merged_df\n",
    "    \"\"\"\n",
    "    soe_weight_value = weights.loc[results_weight[\"Item\"] == \"SoE\", \"Value\"].values[0]\n",
    "    reply_factor_weight_value = weights.loc[results_weight[\"Item\"] == \"ReplyFactor\", \"Value\"].values[0]\n",
    "    relevance_weight_value = weights.loc[results_weight[\"Item\"] == \"relevance\", \"Value\"].values[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    score = (\n",
    "        soe_weight_value * merged_df[\"SoE\"]\n",
    "        + reply_factor_weight_value * merged_df[\"ReplyFactor\"]\n",
    "        +relevance_weight_value * merged_df[\"relevance\"]\n",
    "    )\n",
    "\n",
    "    score = score * merged_df[\"timeNorm\"]\n",
    "\n",
    "    merged_df[\"FinalScore\"] = score\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "741b05ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          commentId     channelId       videoId      authorId  \\\n",
      "count  4.725012e+06  4.725012e+06  4.725012e+06  4.725012e+06   \n",
      "mean   2.362507e+06  2.677102e+04  4.699673e+04  1.820729e+06   \n",
      "std    1.363995e+06  1.503666e+04  2.593627e+04  1.053868e+06   \n",
      "min    0.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    1.181254e+06  1.449200e+04  2.597800e+04  9.060968e+05   \n",
      "50%    2.362506e+06  2.542500e+04  4.709600e+04  1.813632e+06   \n",
      "75%    3.543761e+06  4.061800e+04  6.944500e+04  2.731818e+06   \n",
      "max    4.725015e+06  5.367700e+04  9.285400e+04  3.659440e+06   \n",
      "\n",
      "       parentCommentId     likeCount    dataset_id           SoE  \\\n",
      "count     5.161350e+05  4.725012e+06  4.725012e+06  4.471305e+06   \n",
      "mean      2.623663e+06  1.012744e+01  2.883605e+00  5.110901e-02   \n",
      "std       1.215428e+06  5.444689e+02  1.367501e+00  6.220686e-02   \n",
      "min       5.161510e+05  0.000000e+00  1.000000e+00  0.000000e+00   \n",
      "25%       1.569545e+06  0.000000e+00  2.000000e+00  3.014186e-02   \n",
      "50%       2.625877e+06  0.000000e+00  3.000000e+00  4.414326e-02   \n",
      "75%       3.677645e+06  0.000000e+00  4.000000e+00  6.224738e-02   \n",
      "max       4.725006e+06  4.561420e+05  5.000000e+00  1.180268e+01   \n",
      "\n",
      "        ReplyFactor     relevance      timeNorm    FinalScore  \n",
      "count  4.725012e+06  4.725012e+06  4.725012e+06  4.471305e+06  \n",
      "mean   1.644440e+00  2.485385e-02  6.506442e-01  3.684344e-01  \n",
      "std    3.812591e-01  5.162430e-02  2.345434e-01  1.570157e-01  \n",
      "min    1.500000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "25%    1.500000e+00  0.000000e+00  5.230991e-01  2.824737e-01  \n",
      "50%    1.500000e+00  0.000000e+00  6.794995e-01  3.710964e-01  \n",
      "75%    1.500000e+00  3.117165e-02  8.484119e-01  4.672735e-01  \n",
      "max    4.418865e+00  1.000000e+00  9.754572e-01  1.473182e+00  \n"
     ]
    }
   ],
   "source": [
    "results_final_score = compute_final_score(merged_df, results_weight)\n",
    "print(results_final_score.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
