{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08e5de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6207126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "zip_path = \"/content/drive/MyDrive/dataset.zip\"\n",
    "\n",
    "# Check inside the zip\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    print(z.namelist())   # <-- copy the exact names from here\n",
    "\n",
    "# Open and read CSV directly from the zip\n",
    "with zipfile.ZipFile(zip_path) as z:\n",
    "    with z.open(\"comments1.csv\") as f:\n",
    "        comments_1 = pd.read_csv(f)\n",
    "\n",
    "    with z.open(\"comments2.csv\") as f:\n",
    "        comments_2 = pd.read_csv(f)\n",
    "\n",
    "    with z.open(\"comments3.csv\") as f:\n",
    "        comments_3 = pd.read_csv(f)\n",
    "\n",
    "    with z.open(\"comments4.csv\") as f:   # use exact path from namelist()\n",
    "        comments_4 = pd.read_csv(f)\n",
    "\n",
    "    with z.open(\"comments5.csv\") as f:\n",
    "        comments_5 = pd.read_csv(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec71e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate([comments_1, comments_2, comments_3, comments_4,comments_5], start=1):\n",
    "    df[\"dataset_id\"] = i\n",
    "\n",
    "# Combine\n",
    "df_all_comments = pd.concat([comments_1, comments_2, comments_3, comments_4,comments_5], ignore_index=True)\n",
    "\n",
    "print(df_all_comments.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6bdbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "# ==========================================\n",
    "# 1. Initialize Faster Zero-Shot Classifier\n",
    "# ==========================================\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"typeform/distilbert-base-uncased-mnli\",  # Faster DistilBERT model\n",
    "    device=0                                        # Use GPU if available\n",
    ")\n",
    "\n",
    "labels = [\"skincare\", \"makeup\", \"fragrance\", \"haircare\"]\n",
    "\n",
    "# ==========================================\n",
    "# 2. Batch Processing with Datasets + Checkpoints\n",
    "# ==========================================\n",
    "def classify_comments(\n",
    "    df,\n",
    "    text_column=\"textOriginal\",\n",
    "    batch_size=128,\n",
    "    save_every=1000,   # Save every 1000 rows for efficiency\n",
    "    checkpoint_dir=\"/content/drive/MyDrive/checkpoints/\"\n",
    "):\n",
    "    # Create checkpoint folder if it doesnâ€™t exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_file = os.path.join(checkpoint_dir, \"classified_comments.csv\")\n",
    "\n",
    "    # Drop empty/NaN comments first to save runtime\n",
    "    df = df.copy()\n",
    "    df = df[df[text_column].notna() & (df[text_column].str.strip() != \"\")]\n",
    "    \n",
    "    # Convert to Hugging Face Dataset\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    # Resume from checkpoint if available\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        print(f\"Resuming from checkpoint: {checkpoint_file}\")\n",
    "        df_partial = pd.read_csv(checkpoint_file)\n",
    "        start_idx = len(df_partial)\n",
    "    else:\n",
    "        df_partial = pd.DataFrame(columns=list(df.columns) + [\"predicted_category\", \"confidence\"])\n",
    "        start_idx = 0\n",
    "\n",
    "    # Define mapping function for classification\n",
    "    def classify_batch(batch):\n",
    "        preds = classifier(batch[text_column], candidate_labels=labels, truncation=True)\n",
    "        batch[\"predicted_category\"] = [p[\"labels\"][0] for p in preds]\n",
    "        batch[\"confidence\"] = [p[\"scores\"][0] for p in preds]\n",
    "        return batch\n",
    "\n",
    "    # Process dataset in batches\n",
    "    for i in range(start_idx, len(dataset), batch_size):\n",
    "        batch = dataset.select(range(i, min(i+batch_size, len(dataset))))\n",
    "        batch_result = batch.map(classify_batch, batched=True, batch_size=batch_size)\n",
    "\n",
    "        # Convert batch to DataFrame\n",
    "        batch_df = batch_result.to_pandas()\n",
    "\n",
    "        # Append to partial results\n",
    "        df_partial = pd.concat([df_partial, batch_df], ignore_index=True)\n",
    "\n",
    "        # Save checkpoint every N rows\n",
    "        if (i // batch_size) % (save_every // batch_size) == 0:\n",
    "            df_partial.to_csv(checkpoint_file, index=False)\n",
    "            print(f\"âœ… Checkpoint saved at {len(df_partial)} rows\")\n",
    "\n",
    "        if (i // batch_size) % 50 == 0:\n",
    "            print(f\"Processed {i+batch_size}/{len(dataset)} comments\")\n",
    "\n",
    "    # Final save\n",
    "    df_partial.to_csv(checkpoint_file, index=False)\n",
    "    print(f\"ðŸŽ‰ Final results saved to {checkpoint_file}\")\n",
    "\n",
    "    return df_partial\n",
    "\n",
    "# ==========================================\n",
    "# 3. Run Classification\n",
    "# ==========================================\n",
    "result_df = classify_comments(\n",
    "    df_all_comments,\n",
    "    text_column=\"textOriginal\",\n",
    "    batch_size=128,\n",
    "    save_every=5000,   # Save every ~5000 rows\n",
    "    checkpoint_dir=\"/content/drive/MyDrive/checkpoints/\"\n",
    ")\n",
    "\n",
    "# Preview results\n",
    "print(result_df.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
